{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:19:25.047837Z","iopub.execute_input":"2024-09-16T07:19:25.048245Z","iopub.status.idle":"2024-09-16T07:19:42.248893Z","shell.execute_reply.started":"2024-09-16T07:19:25.048211Z","shell.execute_reply":"2024-09-16T07:19:42.247645Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\nDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from importlib.metadata import version\n\nprint(\"torch version:\", version(\"torch\"))\nprint(\"tiktoken version:\", version(\"tiktoken\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:19:42.251012Z","iopub.execute_input":"2024-09-16T07:19:42.251353Z","iopub.status.idle":"2024-09-16T07:19:42.283369Z","shell.execute_reply.started":"2024-09-16T07:19:42.251318Z","shell.execute_reply":"2024-09-16T07:19:42.282384Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"torch version: 2.1.2+cpu\ntiktoken version: 0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import urllib\n\n# URL of the PDF file\nurl = \"https://ag.gov.np/files/Constitution-of-Nepal_2072_Eng_www.moljpa.gov_.npDate-72_11_16.pdf\"\n\n# Open the URL and download the PDF\nresponse = urllib.request.urlopen(url)\npdf_data = response.read()\n\n# Save the PDF to a local file\nwith open(\"Constitution-of-Nepal_2072_Eng.pdf\", \"wb\") as f:\n    f.write(pdf_data)\n\nprint(\"PDF downloaded and saved as 'Constitution-of-Nepal_2072_Eng.pdf'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:19:42.284572Z","iopub.execute_input":"2024-09-16T07:19:42.284924Z","iopub.status.idle":"2024-09-16T07:19:45.105993Z","shell.execute_reply.started":"2024-09-16T07:19:42.284895Z","shell.execute_reply":"2024-09-16T07:19:45.104737Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"PDF downloaded and saved as 'Constitution-of-Nepal_2072_Eng.pdf'\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install PyPDF2","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:19:45.108837Z","iopub.execute_input":"2024-09-16T07:19:45.109169Z","iopub.status.idle":"2024-09-16T07:20:00.506357Z","shell.execute_reply.started":"2024-09-16T07:19:45.109140Z","shell.execute_reply":"2024-09-16T07:20:00.505064Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import PyPDF2\n\n# Open the downloaded PDF file\nwith open(\"Constitution-of-Nepal_2072_Eng.pdf\", \"rb\") as file:\n    # Initialize a PDF reader object\n    pdf_reader = PyPDF2.PdfReader(file)\n    \n    # Extract text from each page of the PDF\n    raw_text = \"\"\n    for page_num in range(len(pdf_reader.pages)):\n        page = pdf_reader.pages[page_num]\n        raw_text += page.extract_text()\n\n# Print the first 500 characters to preview the content\nprint(raw_text[:50])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:00.508050Z","iopub.execute_input":"2024-09-16T07:20:00.508408Z","iopub.status.idle":"2024-09-16T07:20:09.790732Z","shell.execute_reply.started":"2024-09-16T07:20:00.508371Z","shell.execute_reply":"2024-09-16T07:20:09.789599Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":" \n1 \n  \n \n \n \nTHE CONSTITUTION OF NEPAL  \n   \n2 \n \n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(raw_text))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.794595Z","iopub.execute_input":"2024-09-16T07:20:09.795055Z","iopub.status.idle":"2024-09-16T07:20:09.801056Z","shell.execute_reply.started":"2024-09-16T07:20:09.795022Z","shell.execute_reply":"2024-09-16T07:20:09.799775Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"344782\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Split text into tokens**","metadata":{}},{"cell_type":"code","source":"import re\npreprocessed = re.split(r'([,.:;?_!-\"()\\']|--|\\s|[@#%&])', raw_text)\n\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:30])","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.802515Z","iopub.execute_input":"2024-09-16T07:20:09.802945Z","iopub.status.idle":"2024-09-16T07:20:09.903078Z","shell.execute_reply.started":"2024-09-16T07:20:09.802907Z","shell.execute_reply":"2024-09-16T07:20:09.901943Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['1', 'THE', 'CONSTITUTION', 'OF', 'NEPAL', '2', 'Table', 'of', 'Content', 's', 'Preamble', 'Part-1', 'Preliminary', 'Part-2', 'Citizenship', 'Part-3', 'Fundamental', 'Rights', 'and', 'Duties', 'Part-4', 'Directive', 'Principles', ',', 'Policies', 'and', 'Obligations', 'of', 'the', 'State']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(preprocessed))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.904495Z","iopub.execute_input":"2024-09-16T07:20:09.904879Z","iopub.status.idle":"2024-09-16T07:20:09.917380Z","shell.execute_reply.started":"2024-09-16T07:20:09.904848Z","shell.execute_reply":"2024-09-16T07:20:09.916175Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"65101\n","output_type":"stream"}]},{"cell_type":"code","source":"#Unique tokens\nlen(set(preprocessed))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.918873Z","iopub.execute_input":"2024-09-16T07:20:09.919286Z","iopub.status.idle":"2024-09-16T07:20:09.936959Z","shell.execute_reply.started":"2024-09-16T07:20:09.919246Z","shell.execute_reply":"2024-09-16T07:20:09.935663Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"4525"},"metadata":{}}]},{"cell_type":"markdown","source":"**Create a vocabulary**","metadata":{}},{"cell_type":"code","source":"all_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\n\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.941354Z","iopub.execute_input":"2024-09-16T07:20:09.941765Z","iopub.status.idle":"2024-09-16T07:20:09.954324Z","shell.execute_reply.started":"2024-09-16T07:20:09.941728Z","shell.execute_reply":"2024-09-16T07:20:09.953184Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"4525\n","output_type":"stream"}]},{"cell_type":"code","source":"#Give indexes and put that as dictionary\nvocab = {token:integer for integer,token in enumerate(all_words)}","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.955937Z","iopub.execute_input":"2024-09-16T07:20:09.956353Z","iopub.status.idle":"2024-09-16T07:20:09.968734Z","shell.execute_reply.started":"2024-09-16T07:20:09.956312Z","shell.execute_reply":"2024-09-16T07:20:09.967706Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Simple Tokenization of text****","metadata":{}},{"cell_type":"code","source":"\"\"\"Putting all together\"\"\"\n\nclass SimpleTokenizerV1:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n    \n    def encode(self, text):\n        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n                                \n        preprocessed = [\n            item.strip() for item in preprocessed if item.strip()\n        ]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        # Replace spaces before the specified punctuations\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.969994Z","iopub.execute_input":"2024-09-16T07:20:09.970334Z","iopub.status.idle":"2024-09-16T07:20:09.981375Z","shell.execute_reply.started":"2024-09-16T07:20:09.970305Z","shell.execute_reply":"2024-09-16T07:20:09.980291Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenizer = SimpleTokenizerV1(vocab)\ntokenizer.encode(\"\"\" \"\"\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:09.982818Z","iopub.execute_input":"2024-09-16T07:20:09.983226Z","iopub.status.idle":"2024-09-16T07:20:10.000747Z","shell.execute_reply.started":"2024-09-16T07:20:09.983184Z","shell.execute_reply":"2024-09-16T07:20:09.999329Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Using Bytepair Encoding ","metadata":{}},{"cell_type":"code","source":"import importlib\nimport tiktoken\n\nprint(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:10.002167Z","iopub.execute_input":"2024-09-16T07:20:10.002506Z","iopub.status.idle":"2024-09-16T07:20:10.060419Z","shell.execute_reply.started":"2024-09-16T07:20:10.002476Z","shell.execute_reply":"2024-09-16T07:20:10.059306Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"tiktoken version: 0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = tiktoken.get_encoding(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:10.061975Z","iopub.execute_input":"2024-09-16T07:20:10.062353Z","iopub.status.idle":"2024-09-16T07:20:11.241801Z","shell.execute_reply.started":"2024-09-16T07:20:10.062320Z","shell.execute_reply":"2024-09-16T07:20:11.240619Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"text = (\n    \"No law shall be made providing for the death penalty to any one<|endoftext|> Every citizen\"\n     \"of\"\n)\n\nintegers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n\nprint(integers)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:11.243571Z","iopub.execute_input":"2024-09-16T07:20:11.244351Z","iopub.status.idle":"2024-09-16T07:20:11.250754Z","shell.execute_reply.started":"2024-09-16T07:20:11.244306Z","shell.execute_reply":"2024-09-16T07:20:11.249741Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[2949, 1099, 2236, 307, 925, 4955, 329, 262, 1918, 7389, 284, 597, 530, 50256, 3887, 9511, 1659]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.encode(\"fddfsdfsf\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:11.252041Z","iopub.execute_input":"2024-09-16T07:20:11.252344Z","iopub.status.idle":"2024-09-16T07:20:11.264435Z","shell.execute_reply.started":"2024-09-16T07:20:11.252316Z","shell.execute_reply":"2024-09-16T07:20:11.263405Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[69, 1860, 9501, 7568, 28202]"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode([1860])","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:11.265837Z","iopub.execute_input":"2024-09-16T07:20:11.266132Z","iopub.status.idle":"2024-09-16T07:20:11.275484Z","shell.execute_reply.started":"2024-09-16T07:20:11.266105Z","shell.execute_reply":"2024-09-16T07:20:11.274281Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'dd'"},"metadata":{}}]},{"cell_type":"markdown","source":"**Data Sampling with a sliding window**","metadata":{}},{"cell_type":"markdown","source":"**We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict**","metadata":{}},{"cell_type":"code","source":"import PyPDF2\n# Open the downloaded PDF file\nwith open(\"Constitution-of-Nepal_2072_Eng.pdf\", \"rb\") as file:\n    # Initialize a PDF reader object\n    pdf_reader = PyPDF2.PdfReader(file)\n    \n    # Extract text from each page of the PDF\n    raw_text = \"\"\n    for page_num in range(len(pdf_reader.pages)):\n        page = pdf_reader.pages[page_num]\n        raw_text += page.extract_text()\nenc_text = tokenizer.encode(raw_text)\nprint(len(enc_text))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:11.277132Z","iopub.execute_input":"2024-09-16T07:20:11.277515Z","iopub.status.idle":"2024-09-16T07:20:20.470906Z","shell.execute_reply.started":"2024-09-16T07:20:11.277484Z","shell.execute_reply":"2024-09-16T07:20:20.469733Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"86531\n","output_type":"stream"}]},{"cell_type":"code","source":"\nvocab_size = len(set(enc_text))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:20.472756Z","iopub.execute_input":"2024-09-16T07:20:20.473118Z","iopub.status.idle":"2024-09-16T07:20:20.481860Z","shell.execute_reply.started":"2024-09-16T07:20:20.473086Z","shell.execute_reply":"2024-09-16T07:20:20.480725Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"5305\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:20.483158Z","iopub.execute_input":"2024-09-16T07:20:20.483487Z","iopub.status.idle":"2024-09-16T07:20:23.661426Z","shell.execute_reply.started":"2024-09-16T07:20:20.483458Z","shell.execute_reply":"2024-09-16T07:20:23.660242Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def create_dataloader_v1(txt, batch_size=4, max_length=256, \n                         stride=128, shuffle=True, drop_last=True,\n                         num_workers=0):\n\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers\n    )\n\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:23.662787Z","iopub.execute_input":"2024-09-16T07:20:23.663225Z","iopub.status.idle":"2024-09-16T07:20:23.670579Z","shell.execute_reply.started":"2024-09-16T07:20:23.663196Z","shell.execute_reply":"2024-09-16T07:20:23.669466Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Test a dataloader with a batch size of 1 and context size of 4**","metadata":{}},{"cell_type":"code","source":"dataloader = create_dataloader_v1(\n    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n)\n\ndata_iter = iter(dataloader)\nfirst_batch = next(data_iter)\nprint(first_batch)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:23.671979Z","iopub.execute_input":"2024-09-16T07:20:23.672309Z","iopub.status.idle":"2024-09-16T07:20:25.413848Z","shell.execute_reply.started":"2024-09-16T07:20:23.672280Z","shell.execute_reply":"2024-09-16T07:20:25.412766Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[tensor([[220, 198,  16, 220]]), tensor([[198,  16, 220, 198]])]\n","output_type":"stream"}]},{"cell_type":"code","source":"second_batch = next(data_iter)\nprint(second_batch)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:25.416449Z","iopub.execute_input":"2024-09-16T07:20:25.416815Z","iopub.status.idle":"2024-09-16T07:20:25.423804Z","shell.execute_reply.started":"2024-09-16T07:20:25.416785Z","shell.execute_reply":"2024-09-16T07:20:25.422628Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[tensor([[198,  16, 220, 198]]), tensor([[ 16, 220, 198, 220]])]\n","output_type":"stream"}]},{"cell_type":"code","source":"dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n\ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)\nprint(\"Inputs:\\n\", inputs)\nprint(\"\\nTargets:\\n\", targets)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:25.425245Z","iopub.execute_input":"2024-09-16T07:20:25.425661Z","iopub.status.idle":"2024-09-16T07:20:26.239304Z","shell.execute_reply.started":"2024-09-16T07:20:25.425621Z","shell.execute_reply":"2024-09-16T07:20:26.238022Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Inputs:\n tensor([[  220,   198,    16,   220],\n        [  198,   220,   220,   198],\n        [  220,   198,   220,   198],\n        [  220,   198, 10970,  7102],\n        [ 2257,  2043, 35354,  3963],\n        [  399,  8905,  1847,   220],\n        [  220,   198,   220,   220],\n        [  220,   198,    17,   220]])\n\nTargets:\n tensor([[  198,    16,   220,   198],\n        [  220,   220,   198,   220],\n        [  198,   220,   198,   220],\n        [  198, 10970,  7102,  2257],\n        [ 2043, 35354,  3963,   399],\n        [ 8905,  1847,   220,   220],\n        [  198,   220,   220,   220],\n        [  198,    17,   220,   198]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create token embeddings**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Coding an LLM Architecture","metadata":{}},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n    \"vocab_size\": 5305,    # Vocabulary size from BPE\n    \"context_length\": 1024, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.0,       # Dropout rate\n    \"qkv_bias\": True       # Query-Key-Value bias\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.240864Z","iopub.execute_input":"2024-09-16T07:20:26.241171Z","iopub.status.idle":"2024-09-16T07:20:26.246648Z","shell.execute_reply.started":"2024-09-16T07:20:26.241139Z","shell.execute_reply":"2024-09-16T07:20:26.245590Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Coding the GPT Model**","metadata":{}},{"cell_type":"code","source":"x_2 = inputs[1] # second input element\nd_in = inputs.shape[1] # the input embedding size, d=3\nd_out = 2 # the output embedding size, d=2","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.247865Z","iopub.execute_input":"2024-09-16T07:20:26.248172Z","iopub.status.idle":"2024-09-16T07:20:26.262362Z","shell.execute_reply.started":"2024-09-16T07:20:26.248145Z","shell.execute_reply":"2024-09-16T07:20:26.261198Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.269859Z","iopub.execute_input":"2024-09-16T07:20:26.270223Z","iopub.status.idle":"2024-09-16T07:20:26.277113Z","shell.execute_reply.started":"2024-09-16T07:20:26.270194Z","shell.execute_reply":"2024-09-16T07:20:26.275994Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"tensor([[  220,   198,    16,   220],\n        [  198,   220,   220,   198],\n        [  220,   198,   220,   198],\n        [  220,   198, 10970,  7102],\n        [ 2257,  2043, 35354,  3963],\n        [  399,  8905,  1847,   220],\n        [  220,   198,   220,   220],\n        [  220,   198,    17,   220]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SelfAttention_v1(nn.Module):\n\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n\n    def forward(self, x):\n        # Ensure the input tensor is in float32 format\n        if x.dtype != torch.float32:\n            x = x.float()\n\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n        \n        attn_scores = queries @ keys.T # omega\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n\n        context_vec = attn_weights @ values\n        return context_vec\ntorch.manual_seed(123)\n# Create the SelfAttention_v1 instance\nsa_v1 = SelfAttention_v1(d_in, d_out)\n# Forward pass\nprint(sa_v1(inputs))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.278325Z","iopub.execute_input":"2024-09-16T07:20:26.278666Z","iopub.status.idle":"2024-09-16T07:20:26.356969Z","shell.execute_reply.started":"2024-09-16T07:20:26.278625Z","shell.execute_reply":"2024-09-16T07:20:26.355879Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"tensor([[33748.7266, 28081.3105],\n        [33748.7266, 28081.3105],\n        [33748.7266, 28081.3105],\n        [33748.7266, 28081.3105],\n        [33748.7266, 28081.3105],\n        [33748.7266, 28081.3105],\n        [33748.7266, 28081.3105],\n        [33748.7266, 28081.3105]], grad_fn=<MmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"class SelfAttention_v2(nn.Module):\n\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        if x.dtype != torch.float32:\n            x = x.float()\n\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        \n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.358248Z","iopub.execute_input":"2024-09-16T07:20:26.358588Z","iopub.status.idle":"2024-09-16T07:20:26.375160Z","shell.execute_reply.started":"2024-09-16T07:20:26.358559Z","shell.execute_reply":"2024-09-16T07:20:26.373978Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"tensor([[-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106],\n        [-1047.0568, -1945.5106]], grad_fn=<MmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(sa_v2.W_query))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.376444Z","iopub.execute_input":"2024-09-16T07:20:26.376790Z","iopub.status.idle":"2024-09-16T07:20:26.382895Z","shell.execute_reply.started":"2024-09-16T07:20:26.376760Z","shell.execute_reply":"2024-09-16T07:20:26.381713Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"<class 'torch.nn.modules.linear.Linear'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Applying a causal attention mask**","metadata":{}},{"cell_type":"code","source":"# Reuse the query and key weight matrices of the\n# SelfAttention_v2 object from the previous section for convenience\ninputs = inputs.to(torch.float32)  # Ensure inputs are float32\n\nqueries = sa_v2.W_query(inputs)\nkeys = sa_v2.W_key(inputs) \nattn_scores = queries @ keys.T\n\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.384287Z","iopub.execute_input":"2024-09-16T07:20:26.384780Z","iopub.status.idle":"2024-09-16T07:20:26.397358Z","shell.execute_reply.started":"2024-09-16T07:20:26.384729Z","shell.execute_reply":"2024-09-16T07:20:26.396247Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"tensor([[0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"context_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.398969Z","iopub.execute_input":"2024-09-16T07:20:26.399436Z","iopub.status.idle":"2024-09-16T07:20:26.409592Z","shell.execute_reply.started":"2024-09-16T07:20:26.399362Z","shell.execute_reply":"2024-09-16T07:20:26.408515Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(attn_weights)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.411356Z","iopub.execute_input":"2024-09-16T07:20:26.411809Z","iopub.status.idle":"2024-09-16T07:20:26.418096Z","shell.execute_reply.started":"2024-09-16T07:20:26.411770Z","shell.execute_reply":"2024-09-16T07:20:26.417030Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"tensor([[0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"masked_simple = attn_weights*mask_simple\nprint(masked_simple)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.419902Z","iopub.execute_input":"2024-09-16T07:20:26.420322Z","iopub.status.idle":"2024-09-16T07:20:26.430628Z","shell.execute_reply.started":"2024-09-16T07:20:26.420283Z","shell.execute_reply":"2024-09-16T07:20:26.429463Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.]], grad_fn=<MulBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"row_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.432169Z","iopub.execute_input":"2024-09-16T07:20:26.432561Z","iopub.status.idle":"2024-09-16T07:20:26.441330Z","shell.execute_reply.started":"2024-09-16T07:20:26.432520Z","shell.execute_reply":"2024-09-16T07:20:26.440290Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.]], grad_fn=<DivBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.442761Z","iopub.execute_input":"2024-09-16T07:20:26.443163Z","iopub.status.idle":"2024-09-16T07:20:26.461620Z","shell.execute_reply.started":"2024-09-16T07:20:26.443127Z","shell.execute_reply":"2024-09-16T07:20:26.460448Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"tensor([[-2.1879e+04,        -inf,        -inf,        -inf,        -inf,\n                -inf,        -inf,        -inf],\n        [-1.3480e+04, -1.7075e+04,        -inf,        -inf,        -inf,\n                -inf,        -inf,        -inf],\n        [-1.3992e+04, -1.7244e+04, -1.7778e+04,        -inf,        -inf,\n                -inf,        -inf,        -inf],\n        [ 2.9824e+04, -1.6149e+05, -1.5310e+05, -2.0735e+07,        -inf,\n                -inf,        -inf,        -inf],\n        [ 8.8224e+05, -2.4332e+05, -1.6096e+05, -7.7824e+07, -1.8636e+08,\n                -inf,        -inf,        -inf],\n        [-1.1653e+05, -2.6474e+05, -2.6475e+05, -2.4790e+07, -3.8156e+07,\n          5.6874e+06,        -inf,        -inf],\n        [-1.5006e+04, -1.7674e+04, -1.8277e+04, -1.2530e+06, -1.4423e+06,\n          3.6030e+05, -2.1175e+04,        -inf],\n        [-2.1845e+04, -1.7500e+04, -1.8679e+04, -8.5196e+05, -3.5977e+05,\n          3.3800e+05, -2.1408e+04, -2.1843e+04]],\n       grad_fn=<MaskedFillBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.463063Z","iopub.execute_input":"2024-09-16T07:20:26.463747Z","iopub.status.idle":"2024-09-16T07:20:26.470867Z","shell.execute_reply.started":"2024-09-16T07:20:26.463705Z","shell.execute_reply":"2024-09-16T07:20:26.469751Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0.]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Masking additional attention weights with dropout**","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\ndropout = torch.nn.Dropout(0.5) # dropout rate of 50%\nexample = torch.ones(6, 6) # create a matrix of ones\n\nprint(dropout(example))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.472157Z","iopub.execute_input":"2024-09-16T07:20:26.472453Z","iopub.status.idle":"2024-09-16T07:20:26.489831Z","shell.execute_reply.started":"2024-09-16T07:20:26.472426Z","shell.execute_reply":"2024-09-16T07:20:26.488749Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"tensor([[2., 2., 2., 2., 2., 2.],\n        [0., 2., 0., 0., 0., 0.],\n        [0., 0., 2., 0., 2., 0.],\n        [2., 2., 0., 0., 0., 2.],\n        [2., 0., 0., 0., 0., 2.],\n        [0., 2., 0., 0., 0., 0.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(123)\nprint(dropout(attn_weights))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.491754Z","iopub.execute_input":"2024-09-16T07:20:26.492183Z","iopub.status.idle":"2024-09-16T07:20:26.499901Z","shell.execute_reply.started":"2024-09-16T07:20:26.492126Z","shell.execute_reply":"2024-09-16T07:20:26.498648Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"tensor([[2., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [2., 0., 0., 0., 0., 0., 0., 0.],\n        [2., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 2., 0., 0.]], grad_fn=<MulBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Now, we are ready to implement a working implementation of self-attention, including the causal and dropout masks**","metadata":{}},{"cell_type":"code","source":"batch = torch.stack((inputs, inputs), dim=0)\nprint(batch.shape) # 2 inputs with 8 tokens each, and each token has embedding dimension 4","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.501259Z","iopub.execute_input":"2024-09-16T07:20:26.501677Z","iopub.status.idle":"2024-09-16T07:20:26.509388Z","shell.execute_reply.started":"2024-09-16T07:20:26.501646Z","shell.execute_reply":"2024-09-16T07:20:26.508402Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"torch.Size([2, 8, 4])\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"Causal attention means that the model cannot look at the future,\nbut can take a look back\"\"\"\nclass CausalAttention(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length,\n                 dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout) # New\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # New batch dimension b\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n        attn_scores.masked_fill_(  # New, _ ops are in-place\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n        attn_weights = self.dropout(attn_weights) # New\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\n\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\n\ncontext_vecs = ca(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.510758Z","iopub.execute_input":"2024-09-16T07:20:26.511085Z","iopub.status.idle":"2024-09-16T07:20:26.533132Z","shell.execute_reply.started":"2024-09-16T07:20:26.511057Z","shell.execute_reply":"2024-09-16T07:20:26.531911Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"tensor([[[ 1.4331e+01,  2.0460e+02],\n         [ 2.3399e+00,  2.1614e+02],\n         [-1.3254e+01,  2.2187e+02],\n         [-1.7280e+02,  5.0237e+03],\n         [-3.7248e+03,  7.7348e+03],\n         [ 1.4331e+01,  2.0460e+02],\n         [-3.7248e+03,  7.7348e+03],\n         [-3.7248e+03,  7.7348e+03]],\n\n        [[ 1.4331e+01,  2.0460e+02],\n         [ 2.3399e+00,  2.1614e+02],\n         [-1.3254e+01,  2.2187e+02],\n         [-1.7280e+02,  5.0237e+03],\n         [-3.7248e+03,  7.7348e+03],\n         [ 1.4331e+01,  2.0460e+02],\n         [-3.7248e+03,  7.7348e+03],\n         [-3.7248e+03,  7.7348e+03]]], grad_fn=<UnsafeViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 8, 2])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Extending single-head attention to multi-head attention\n**Stacking multiple single-head attention layers**","metadata":{}},{"cell_type":"code","source":"class CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        b, num_tokens, d_in = x.shape  # New batch dimension b\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        \n        # Example of attention mechanism (scaled dot-product attention)\n        attention_scores = torch.bmm(queries, keys.transpose(1, 2)) / (d_in ** 0.5)\n        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        attended_values = torch.bmm(attention_weights, values)\n        \n        return attended_values  # Output dimension: (b, num_tokens, d_out)\n\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [CausalAttention(d_in, d_out, dropout, qkv_bias) for _ in range(num_heads)]\n        )\n    \n    def forward(self, x):\n        # Apply each attention head and concatenate the results\n        head_outputs = [head(x) for head in self.heads]\n        concat_output = torch.cat(head_outputs, dim=-1)  # Concatenate along the feature dimension\n        return concat_output\n\n# Example usage\ntorch.manual_seed(123)\n\nbatch = torch.randn(16, 10, 3)  # Example batch with shape (batch_size, context_length, d_in)\ncontext_length = batch.shape[1]\nd_in, d_out = 3, 2\nnum_heads = 2\n\nmha = MultiHeadAttentionWrapper(\n    d_in, d_out, dropout=0.0, num_heads=num_heads\n)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.534557Z","iopub.execute_input":"2024-09-16T07:20:26.534989Z","iopub.status.idle":"2024-09-16T07:20:26.567523Z","shell.execute_reply.started":"2024-09-16T07:20:26.534952Z","shell.execute_reply":"2024-09-16T07:20:26.566476Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"tensor([[[ 3.4373e-02,  3.4932e-02, -1.8032e-02,  2.7194e-02],\n         [ 5.6516e-02,  1.2576e-02, -1.2246e-02, -2.2201e-03],\n         [ 4.3832e-02,  2.4913e-02, -8.2159e-03, -2.6836e-02],\n         [ 7.0120e-02,  3.9669e-02, -3.9439e-02,  1.1509e-01],\n         [ 5.6048e-02, -2.2458e-02, -2.1695e-02,  4.5938e-02],\n         [ 3.6287e-02,  4.3885e-02, -2.6986e-02,  6.6795e-02],\n         [ 5.5130e-02, -1.7051e-02, -7.5909e-03, -3.0422e-02],\n         [ 2.8773e-02, -1.8578e-03, -2.5042e-02,  6.0119e-02],\n         [ 2.1483e-02,  4.3345e-02, -3.3426e-03, -6.4934e-02],\n         [ 5.6056e-02,  5.0708e-02, -8.6930e-03, -2.4235e-02]],\n\n        [[-4.1947e-02,  9.0336e-02,  1.0794e-01, -1.5351e-01],\n         [-1.2100e-01, -3.1050e-04,  1.4609e-01, -2.9761e-01],\n         [-8.7553e-02,  6.6382e-03,  1.0577e-01, -1.3918e-01],\n         [-6.8727e-02, -7.0204e-02,  1.2272e-01, -2.1260e-01],\n         [-1.3041e-01, -3.2071e-02,  1.0984e-01, -1.5795e-01],\n         [-9.9118e-02, -2.5848e-02,  9.6548e-02, -8.3819e-02],\n         [-1.2179e-01, -4.6118e-02,  8.1016e-02,  4.7535e-02],\n         [-1.6143e-01, -3.1471e-02,  1.4267e-01, -2.8540e-01],\n         [-1.5254e-01, -6.5987e-02,  1.4700e-01, -2.9758e-01],\n         [-8.6071e-02, -3.2770e-02,  1.2007e-01, -2.0346e-01]],\n\n        [[-9.7510e-02,  2.9843e-02,  5.9200e-02, -4.3063e-02],\n         [-1.3190e-01, -2.8260e-02,  5.9088e-02, -4.7137e-02],\n         [-1.0451e-01,  3.7284e-02,  5.9026e-02, -4.1365e-02],\n         [-1.5678e-01,  3.1955e-02,  6.5130e-02, -1.0188e-01],\n         [-1.2579e-01,  3.1866e-03,  3.1417e-02,  7.8248e-02],\n         [-1.1364e-01,  1.0590e-02,  5.5248e-02, -2.3399e-02],\n         [-1.2328e-01,  3.5117e-02,  6.2639e-02, -6.4024e-02],\n         [-1.1327e-01, -2.8173e-02,  5.4844e-02, -2.3961e-02],\n         [-1.0528e-01,  4.4095e-03,  4.5886e-02,  1.9356e-02],\n         [-5.7157e-02,  7.2747e-02,  5.0482e-02,  2.8819e-03]],\n\n        [[-9.6168e-02,  1.3442e-02, -2.2610e-03,  4.6262e-02],\n         [-7.8097e-02, -1.7165e-02, -1.2515e-01,  3.7991e-01],\n         [-7.5603e-02,  1.0905e-01, -1.0293e-01,  3.1876e-01],\n         [-9.2609e-02, -2.4569e-02, -2.0928e-02,  9.5863e-02],\n         [-8.8278e-02, -1.0218e-01,  1.2838e-01, -2.8679e-01],\n         [-9.7940e-02, -1.0667e-02,  1.2895e-02,  6.3605e-03],\n         [-6.9872e-02,  5.3422e-03, -7.4611e-02,  2.4077e-01],\n         [-9.5563e-02,  2.0739e-02,  1.3791e-02,  3.9557e-03],\n         [-9.2867e-02, -4.5496e-02,  5.3579e-02, -9.9209e-02],\n         [-8.6697e-02,  1.2745e-03, -3.6187e-02,  1.3670e-01]],\n\n        [[-1.5232e-01,  6.5006e-02,  2.2938e-02,  2.5356e-02],\n         [-1.4101e-01, -2.8840e-02,  4.4505e-02, -4.9835e-02],\n         [-1.1209e-01,  1.5180e-01,  3.1300e-02, -1.0864e-03],\n         [-9.0656e-02,  1.1322e-01, -1.0472e-02,  1.4469e-01],\n         [-1.0215e-01,  7.4679e-02, -1.1943e-02,  1.4882e-01],\n         [-1.4340e-01,  1.1407e-01,  1.1383e-02,  6.6776e-02],\n         [-1.1001e-01, -2.6040e-02, -1.8887e-02,  1.7110e-01],\n         [-1.3364e-01,  6.3947e-02,  1.5565e-02,  5.0991e-02],\n         [-9.6096e-02,  2.6066e-02, -1.4253e-02,  1.5584e-01],\n         [-1.3494e-01,  2.6535e-02, -1.4101e-02,  1.5514e-01]],\n\n        [[ 6.3917e-02,  5.3121e-02, -9.9019e-02,  1.8194e-01],\n         [ 7.7839e-02,  1.2716e-01, -9.9818e-02,  1.8768e-01],\n         [ 6.3411e-02,  5.2023e-02, -9.3026e-02,  1.4254e-01],\n         [ 1.1655e-01, -3.5621e-02, -8.2368e-02,  7.3349e-02],\n         [ 1.0152e-01,  6.9429e-02, -1.1487e-01,  3.2169e-01],\n         [ 1.3036e-01,  2.3442e-02, -9.5445e-02,  1.5807e-01],\n         [ 7.4358e-02,  1.6751e-01, -8.4237e-02,  8.7460e-02],\n         [ 3.7175e-02,  8.1857e-02, -1.0487e-01,  2.2338e-01],\n         [ 9.1496e-02, -7.1089e-02, -1.0544e-01,  2.2713e-01],\n         [ 9.4935e-02,  3.4351e-02, -9.8224e-02,  1.7650e-01]],\n\n        [[-7.7532e-02, -2.0833e-01,  6.0579e-02, -1.2618e-01],\n         [-5.2588e-02, -6.2417e-02,  6.0419e-02, -1.1541e-01],\n         [-1.2741e-01,  9.5693e-02,  5.8489e-02, -8.2110e-02],\n         [-4.1889e-02, -6.5964e-02,  5.2832e-02, -4.8847e-02],\n         [-8.1910e-02,  1.3781e-01,  6.5169e-02, -1.3329e-01],\n         [-2.7717e-02, -4.1825e-02,  4.8512e-02, -5.1238e-04],\n         [-1.4855e-02,  4.5985e-02,  7.0473e-02, -1.8073e-01],\n         [-3.8620e-02, -8.8122e-03,  6.2669e-02, -1.2874e-01],\n         [-6.7145e-02, -1.6614e-02,  5.6114e-02, -7.3350e-02],\n         [-4.3532e-02, -1.3667e-02,  5.8317e-02, -9.3011e-02]],\n\n        [[ 3.9051e-02,  1.1978e-01, -1.0363e-01,  2.2909e-01],\n         [ 2.4146e-02,  1.2573e-01, -8.1671e-02,  1.9851e-01],\n         [ 4.4272e-02,  8.2349e-02, -7.3746e-02,  1.8903e-01],\n         [ 3.1177e-02,  8.2460e-02, -8.4559e-02,  2.0423e-01],\n         [ 4.2537e-02,  8.1325e-02, -9.3899e-02,  2.1741e-01],\n         [ 2.8814e-02,  1.2160e-01, -9.1301e-02,  2.1213e-01],\n         [ 3.4304e-02,  1.2259e-01, -1.0159e-01,  2.2617e-01],\n         [ 2.6479e-02,  8.2074e-02, -7.8124e-02,  1.9505e-01],\n         [ 3.6136e-02,  1.2853e-01, -6.9927e-02,  1.8159e-01],\n         [ 3.1992e-02,  1.2450e-01, -7.4518e-02,  1.8842e-01]],\n\n        [[-1.1385e-01,  9.6018e-02, -4.2744e-02,  1.6891e-01],\n         [-7.3978e-02, -1.0297e-02, -2.9244e-02,  9.1400e-02],\n         [-3.5980e-02,  1.4603e-01, -6.3750e-02,  2.7357e-01],\n         [-4.1244e-02,  9.7890e-02, -2.2620e-02,  5.8948e-02],\n         [-3.8404e-02,  6.4881e-02, -6.4267e-02,  2.7631e-01],\n         [-6.6044e-02,  9.3697e-02, -4.8984e-02,  2.0094e-01],\n         [-1.1213e-02,  1.7147e-01, -1.8134e-02,  3.9369e-02],\n         [-2.2765e-02,  1.1401e-01, -4.4300e-02,  1.7771e-01],\n         [ 1.1369e-02,  1.5610e-01, -9.2438e-02,  4.0512e-01],\n         [-1.0290e-01,  1.1837e-01, -4.2706e-02,  1.6943e-01]],\n\n        [[-2.1639e-01, -3.7982e-03,  1.0392e-01, -1.4224e-01],\n         [-2.2356e-01, -4.4817e-02,  1.2231e-01, -9.1746e-02],\n         [-1.7030e-01, -2.2158e-02,  1.2488e-01, -8.6634e-02],\n         [-1.9243e-01, -1.2409e-02,  1.1525e-01, -1.1193e-01],\n         [-1.9876e-01, -3.1244e-02,  1.1965e-01, -9.9340e-02],\n         [-1.8555e-01, -1.3743e-02,  1.1677e-01, -1.0796e-01],\n         [-1.9455e-01, -2.3029e-02,  1.0972e-01, -1.2432e-01],\n         [-1.7671e-01,  1.5992e-02,  1.1584e-01, -1.1340e-01],\n         [-2.2245e-01, -1.8793e-02,  1.1488e-01, -1.1275e-01],\n         [-1.8625e-01,  1.0369e-02,  1.1938e-01, -1.0426e-01]],\n\n        [[-9.6227e-03,  1.4852e-01, -7.0439e-02,  2.1788e-01],\n         [-2.0738e-02,  1.0285e-01, -6.0554e-02,  1.8394e-01],\n         [-1.0738e-02,  1.3498e-01, -7.0347e-02,  2.1736e-01],\n         [-4.9083e-03,  1.4639e-01, -7.7719e-02,  2.4435e-01],\n         [-4.3334e-02,  9.6553e-02, -6.0287e-02,  1.8316e-01],\n         [-4.0098e-02,  7.7093e-02, -6.2124e-02,  1.8878e-01],\n         [-1.6333e-02,  1.0442e-01, -7.4997e-02,  2.3357e-01],\n         [ 9.0646e-03,  1.6545e-01, -6.1662e-02,  1.8843e-01],\n         [ 2.6199e-03,  1.5066e-01, -8.2550e-02,  2.6409e-01],\n         [-1.5039e-03,  1.2942e-01, -7.5346e-02,  2.3513e-01]],\n\n        [[ 1.0141e-01, -8.6489e-02, -4.1942e-02, -3.2493e-02],\n         [ 1.0430e-01, -2.8643e-02, -3.4511e-02, -1.0388e-02],\n         [ 9.3669e-02, -2.3236e-03, -2.9731e-02,  3.9878e-03],\n         [ 8.5052e-02, -3.2383e-02, -2.3871e-02,  2.5407e-02],\n         [ 7.3696e-02, -2.2927e-02, -3.6849e-02, -2.2940e-02],\n         [ 1.2540e-01, -4.3128e-02, -8.4165e-03,  6.3756e-02],\n         [ 1.5785e-01,  5.3711e-02, -3.6970e-02, -3.3848e-02],\n         [ 6.6595e-02, -8.6153e-02, -3.2846e-02,  3.9109e-03],\n         [ 1.6567e-01,  7.1352e-02, -3.5490e-02, -2.8131e-02],\n         [ 1.4170e-02, -6.6271e-02, -4.1872e-02, -4.1057e-02]],\n\n        [[-4.7675e-02,  1.2454e-01, -1.4597e-01,  3.5089e-01],\n         [-6.0834e-02,  1.5073e-01, -1.5885e-01,  4.4160e-01],\n         [-5.6166e-02,  1.6745e-01, -1.6142e-01,  4.5475e-01],\n         [-4.1898e-02,  1.0094e-01, -1.6693e-01,  4.8320e-01],\n         [-6.1221e-02,  1.1709e-01, -1.5342e-01,  4.1024e-01],\n         [-5.7661e-02,  1.5064e-01, -1.5157e-01,  3.9642e-01],\n         [-7.9723e-02,  1.6182e-01, -1.6733e-01,  4.8365e-01],\n         [-4.3256e-02,  1.5248e-01, -1.5374e-01,  4.1100e-01],\n         [-8.5048e-02,  1.6947e-01, -1.4822e-01,  3.6980e-01],\n         [-5.4011e-02,  1.0227e-01, -1.5666e-01,  4.3054e-01]],\n\n        [[ 7.2949e-02, -1.5841e-01,  1.3868e-01, -4.1400e-01],\n         [ 7.3893e-02, -1.4669e-01,  1.6692e-01, -5.1486e-01],\n         [ 6.3354e-02, -1.3121e-01,  2.1127e-01, -6.8174e-01],\n         [ 1.0663e-01, -2.5356e-01,  1.0730e-01, -3.0682e-01],\n         [ 6.4506e-02, -9.7837e-02,  1.5540e-01, -4.7308e-01],\n         [ 1.1106e-01, -2.3214e-01,  1.0002e-01, -2.8266e-01],\n         [ 9.3221e-02, -1.2095e-01,  1.2898e-01, -3.8029e-01],\n         [ 1.3351e-01, -1.3744e-01,  6.5888e-02, -1.7270e-01],\n         [ 4.3460e-02, -2.0311e-01,  1.5162e-01, -4.5982e-01],\n         [ 7.3016e-02, -2.0550e-01,  1.9290e-01, -6.1178e-01]],\n\n        [[ 6.4976e-02,  7.8277e-02, -4.5628e-02,  9.9086e-02],\n         [ 6.1600e-02,  7.7655e-02, -4.8032e-02,  1.0411e-01],\n         [ 6.5277e-02,  9.2286e-02, -3.3532e-02,  7.2730e-02],\n         [ 6.5918e-02,  6.8079e-02, -7.9068e-02,  1.6860e-01],\n         [ 5.4721e-02,  5.0887e-02, -6.6256e-02,  1.4264e-01],\n         [ 5.0296e-02,  8.5925e-02, -3.6952e-02,  7.9905e-02],\n         [ 6.4070e-02,  1.1697e-01, -8.9554e-02,  1.8816e-01],\n         [ 6.0848e-02,  7.0070e-02, -6.5693e-02,  1.4115e-01],\n         [ 6.4504e-02,  1.2854e-01, -1.1210e-01,  2.3195e-01],\n         [ 5.5239e-02,  5.9495e-02, -3.6092e-02,  7.8802e-02]],\n\n        [[ 1.0938e-01, -1.8904e-01,  1.0466e-01, -4.2353e-01],\n         [ 1.0489e-01, -1.5534e-01,  1.0444e-01, -4.2491e-01],\n         [ 1.2645e-01, -1.6579e-01,  1.0491e-01, -4.2252e-01],\n         [ 1.5189e-01, -1.4650e-01,  1.0387e-01, -4.2788e-01],\n         [ 1.3558e-01, -1.8081e-01,  1.2870e-01, -3.8736e-01],\n         [ 7.7489e-02, -2.5336e-01,  1.0272e-01, -4.3801e-01],\n         [ 1.3564e-01, -1.6467e-01,  1.1391e-01, -4.0022e-01],\n         [ 1.1776e-01, -2.2489e-01,  1.2294e-01, -3.8880e-01],\n         [ 1.3119e-01, -1.7058e-01,  1.0942e-01, -4.0816e-01],\n         [ 9.9579e-02, -1.6876e-01,  1.1991e-01, -3.9375e-01]]],\n       grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([16, 10, 4])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Implementing multi-head attention with weight splits","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n        \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2) \n        \n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec\n\ntorch.manual_seed(123)\n\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:26.569141Z","iopub.execute_input":"2024-09-16T07:20:26.569478Z","iopub.status.idle":"2024-09-16T07:20:26.599066Z","shell.execute_reply.started":"2024-09-16T07:20:26.569448Z","shell.execute_reply":"2024-09-16T07:20:26.597973Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"tensor([[[ 0.1829,  0.6533],\n         [ 0.2044,  0.6853],\n         [ 0.2343,  0.7194],\n         [ 0.0991,  0.7718],\n         [ 0.1481,  0.6130],\n         [ 0.1328,  0.6741],\n         [ 0.1988,  0.5684],\n         [ 0.1820,  0.5086],\n         [ 0.2775,  0.5498],\n         [ 0.2107,  0.6883]],\n\n        [[ 0.1155,  1.5834],\n         [ 0.3182,  1.2603],\n         [ 0.2205,  1.2699],\n         [ 0.2575,  1.0431],\n         [ 0.2662,  0.9379],\n         [ 0.1915,  0.9777],\n         [ 0.1005,  0.8624],\n         [ 0.3509,  0.7102],\n         [ 0.3911,  0.5504],\n         [ 0.2749,  0.7494]],\n\n        [[ 0.2119,  1.0297],\n         [ 0.2763,  0.6831],\n         [ 0.2420,  0.8347],\n         [ 0.4347,  0.7683],\n         [ 0.2326,  0.7210],\n         [ 0.2635,  0.7271],\n         [ 0.3182,  0.7598],\n         [ 0.2509,  0.7100],\n         [ 0.2235,  0.7215],\n         [ 0.1617,  0.9823]],\n\n        [[ 0.2018,  0.6495],\n         [ 0.0435,  0.7165],\n         [-0.1398,  1.4743],\n         [ 0.0688,  0.8622],\n         [ 0.2684,  0.5365],\n         [ 0.2063,  0.6413],\n         [-0.0230,  1.2061],\n         [ 0.1586,  0.7916],\n         [ 0.2191,  0.6432],\n         [ 0.1239,  0.8507]],\n\n        [[ 0.3127,  0.5516],\n         [ 0.3786,  0.3936],\n         [ 0.2153,  1.0278],\n         [ 0.1187,  1.3046],\n         [ 0.1309,  1.1819],\n         [ 0.2009,  0.9784],\n         [ 0.1689,  0.9345],\n         [ 0.2126,  0.8557],\n         [ 0.1348,  0.9996],\n         [ 0.1876,  0.8219]],\n\n        [[ 0.1808,  0.5003],\n         [ 0.1140,  0.8272],\n         [ 0.1779,  0.6876],\n         [ 0.2303,  0.6547],\n         [-0.0371,  0.6686],\n         [ 0.1043,  0.7255],\n         [ 0.1724,  0.8781],\n         [ 0.1361,  0.7592],\n         [ 0.1344,  0.5598],\n         [ 0.1295,  0.6781]],\n\n        [[ 0.3782,  0.1330],\n         [ 0.3474,  0.3002],\n         [ 0.3221,  0.3385],\n         [ 0.2758,  0.3864],\n         [ 0.2261,  0.7070],\n         [ 0.1743,  0.6729],\n         [ 0.2144,  0.7965],\n         [ 0.2522,  0.7074],\n         [ 0.2562,  0.6535],\n         [ 0.2405,  0.6947]],\n\n        [[ 0.3391,  1.0985],\n         [ 0.2525,  0.9132],\n         [ 0.1050,  0.7552],\n         [ 0.1724,  0.7118],\n         [ 0.1974,  0.7263],\n         [ 0.2238,  0.7592],\n         [ 0.2577,  0.8047],\n         [ 0.1934,  0.7331],\n         [ 0.1526,  0.7432],\n         [ 0.1575,  0.7382]],\n\n        [[ 0.2939,  0.3381],\n         [ 0.3440,  0.2050],\n         [ 0.1846,  0.6910],\n         [ 0.2737,  0.5913],\n         [ 0.1953,  0.6770],\n         [ 0.2237,  0.6207],\n         [ 0.2176,  0.8959],\n         [ 0.1856,  0.9099],\n         [ 0.0423,  1.1787],\n         [ 0.2280,  0.7569]],\n\n        [[ 0.4839,  1.0950],\n         [ 0.3305,  0.5995],\n         [ 0.1663,  0.7072],\n         [ 0.3105,  0.7794],\n         [ 0.2410,  0.7152],\n         [ 0.2611,  0.7739],\n         [ 0.3205,  0.7874],\n         [ 0.2917,  0.9013],\n         [ 0.3141,  0.8133],\n         [ 0.2675,  0.8989]],\n\n        [[ 0.1977,  0.8583],\n         [ 0.2472,  0.8418],\n         [ 0.2279,  0.8452],\n         [ 0.1884,  0.8366],\n         [ 0.2490,  0.7217],\n         [ 0.2609,  0.6734],\n         [ 0.2132,  0.7191],\n         [ 0.2360,  0.7962],\n         [ 0.1544,  0.8308],\n         [ 0.1602,  0.8255]],\n\n        [[ 0.2658,  0.4740],\n         [ 0.2177,  0.5589],\n         [ 0.1888,  0.5788],\n         [ 0.1585,  0.5298],\n         [ 0.1805,  0.5296],\n         [ 0.0210,  0.4566],\n         [ 0.1528,  0.6343],\n         [ 0.1166,  0.4941],\n         [ 0.1505,  0.6782],\n         [ 0.2441,  0.4361]],\n\n        [[ 0.2832,  1.0198],\n         [ 0.1491,  0.9843],\n         [ 0.0543,  1.1382],\n         [-0.0029,  0.9451],\n         [ 0.0851,  0.8296],\n         [ 0.0821,  0.9389],\n         [ 0.0341,  0.9265],\n         [ 0.0411,  1.0055],\n         [ 0.1166,  0.9632],\n         [ 0.0831,  0.8848]],\n\n        [[ 0.2574,  0.3990],\n         [ 0.2915,  0.5024],\n         [ 0.3935,  0.6200],\n         [ 0.3024,  0.3510],\n         [ 0.3331,  0.5190],\n         [ 0.2502,  0.3966],\n         [ 0.2455,  0.5510],\n         [ 0.1053,  0.7016],\n         [ 0.3746,  0.1576],\n         [ 0.3938,  0.1906]],\n\n        [[ 0.2509,  0.8881],\n         [ 0.2575,  0.8185],\n         [ 0.2784,  0.8928],\n         [ 0.1956,  0.8257],\n         [ 0.2245,  0.7212],\n         [ 0.2962,  0.6636],\n         [ 0.1898,  0.7152],\n         [ 0.2079,  0.6817],\n         [ 0.1215,  0.7088],\n         [ 0.2437,  0.6637]],\n\n        [[ 0.3485,  0.3623],\n         [ 0.3603,  0.4662],\n         [ 0.3431,  0.5156],\n         [ 0.3315,  0.6174],\n         [ 0.1734,  0.5099],\n         [ 0.3959,  0.1921],\n         [ 0.2769,  0.4321],\n         [ 0.1734,  0.3145],\n         [ 0.2732,  0.3631],\n         [ 0.2493,  0.3439]]], grad_fn=<ViewBackward0>)\ncontext_vecs.shape: torch.Size([16, 10, 2])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pretraining LLMs","metadata":{}},{"cell_type":"code","source":"from importlib.metadata import version\n\npkgs = [\"matplotlib\", \n        \"numpy\", \n        \"tiktoken\", \n        \"torch\",\n        \"tensorflow\" # For OpenAI's pretrained weights\n       ]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:20:45.748539Z","iopub.execute_input":"2024-09-16T07:20:45.748935Z","iopub.status.idle":"2024-09-16T07:20:45.769017Z","shell.execute_reply.started":"2024-09-16T07:20:45.748903Z","shell.execute_reply":"2024-09-16T07:20:45.767755Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"matplotlib version: 3.7.5\nnumpy version: 1.26.4\ntiktoken version: 0.7.0\ntorch version: 2.1.2+cpu\ntensorflow version: 2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GPT Model","metadata":{}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],\n            dropout=cfg[\"drop_rate\"],\n            qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # Shortcut connection for attention block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        # Shortcut connection for feed-forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        return x\n\n\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:31:14.704998Z","iopub.execute_input":"2024-09-16T07:31:14.705518Z","iopub.status.idle":"2024-09-16T07:31:14.733886Z","shell.execute_reply.started":"2024-09-16T07:31:14.705474Z","shell.execute_reply":"2024-09-16T07:31:14.732562Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import torch\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 256, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # Embedding dimension\n    \"n_heads\": 12,         # Number of attention heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value bias\n}\n\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval();  # Disable dropout during inference","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:31:18.566673Z","iopub.execute_input":"2024-09-16T07:31:18.567102Z","iopub.status.idle":"2024-09-16T07:31:20.434187Z","shell.execute_reply.started":"2024-09-16T07:31:18.567071Z","shell.execute_reply":"2024-09-16T07:31:20.433233Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def generate_text_simple(model, idx, max_new_tokens, context_size):\n    # idx is (batch, n_tokens) array of indices in the current context\n    for _ in range(max_new_tokens):\n        \n        # Crop current context if it exceeds the supported context size\n        # E.g., if LLM supports only 5 tokens, and the context size is 10\n        # then only the last 5 tokens are used as context\n        idx_cond = idx[:, -context_size:]\n        \n        # Get the predictions\n        with torch.no_grad():\n            logits = model(idx_cond)\n        \n        # Focus only on the last time step\n        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n        logits = logits[:, -1, :]  \n\n        # Apply softmax to get probabilities\n        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n\n        # Get the idx of the vocab entry with the highest probability value\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n\n        # Append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n\n    return idx","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:38:00.049887Z","iopub.execute_input":"2024-09-16T07:38:00.050294Z","iopub.status.idle":"2024-09-16T07:38:00.058890Z","shell.execute_reply.started":"2024-09-16T07:38:00.050264Z","shell.execute_reply":"2024-09-16T07:38:00.057757Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\nstart_context = \"To be citizens of Nepal\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:39:42.006867Z","iopub.execute_input":"2024-09-16T07:39:42.007274Z","iopub.status.idle":"2024-09-16T07:39:42.848760Z","shell.execute_reply.started":"2024-09-16T07:39:42.007243Z","shell.execute_reply":"2024-09-16T07:39:42.847623Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Output text:\n To be citizens of NepalAPS instabilityINTON niftyhold temporary winkRoman Dirt leaf\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}